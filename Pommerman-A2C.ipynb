{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pommerman\n",
    "\n",
    "The Pommerman  challenge was originally held on June 3rd, 2018, where the goal was to train an agent to play the famous Bomberman game from Nintendo. In this competition, the agent had to face three other agents in a free-for-all game. The holders of the competition designed an environment based on the original game and invited everyone to submit their agents. On November 21st a similar competition was held. This time the agent had to learn how to cooperate with a teammate facing another team consisting of two agents.\n",
    "We decided to approach this challenge using the advantage actor critic ([A2C](https://hackernoon.com/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752)) method, with the goal to train two separate agents to perform better than two hardcoded agents. Each match starts on a randomly drawn 11x11 grid. There are four agents, one in each corner. The agent's teammate is on the kitty corner.\n",
    "\n",
    "In addition to the agents, the board contains wood walls and rigid walls. Rigid walls are indestructible. Wooden walls can be destroyed by bombs. After they are destroyed, they become either a passage or a power-up 50 \\% of the times. The agent starts with one bomb. Every time it lays a bomb, its count decreases by one. After that bomb explodes, the count will increase by one. If the agent picks up the **Extra Bomb** power up, its count will increase by one. The agent also has a blast strength that starts at three, which is how far in the vertical and horizontal directions that bomb will effect. If the **Increase Range** power up is picked up, its range will increase by one. If an agent picks up the **Can Kick** power up, the agent is allowed to kick a bomb by running into it. A bomb has a fuse of 10 time steps. After that, it explodes and any wooden walls, agents, power-ups or other bombs in its range  are destroyed. For each timestep any agent can choose from these six actions\n",
    "\n",
    "* **Stop** : This action is a pass.\n",
    "* **Up**: Move one position up on the board.\n",
    "* **Left**: Move one position to the left on the board.\n",
    "* **Down**: Move one position down on the board.\n",
    "* **Right**: Move one position to the right on the board.\n",
    "* **Bomb**: Lay a bomb at the current position.\n",
    "\n",
    "The agents receive the following observations on each timestep\n",
    "\n",
    "* **Board**: 121 ints representing the flattened board. All squares ouside of the agents view will be covered by fog: int value 5\n",
    "* **Position**: 2 ints (x, y) each in the range [0, 10].\n",
    "* **Ammo**: 1 int showing the agent's current ammo. How many bombs the agent has left to place.\n",
    "* **Blast Strength**: 1 int the blast radius of the agent's bomb.\n",
    "* **Can Kick**: 1 in , 0 or 1. Indiates whether or not the agent can kick the bomb.\n",
    "* **Teammate**: 1 int int the range [-1, 3]. In the free for all environment this will be -1. In the cooperation environment it will be the id of the agent's teammate.\n",
    "* **Enemies**: 3 ints in the range [-1, 3]. The ids for each enemy. In the cooperation environment the last id will be -1, as there are only 2 enemies.\n",
    "* **Bombs**: List of ints. The bombs visible to the agent. Contains X int, Y int and Blast strength for each bomb.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this after a change in the Pommerman folder\n",
    "!py -m pip install -U . --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pommerman\n",
    "from pommerman import agents\n",
    "from pommerman.agents import BaseAgent\n",
    "from pommerman import characters\n",
    "import pommerman.envs\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "def get_variable(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "gamma = 0.99\n",
    "lr = 2.5e-4\n",
    "eps = 1e-5\n",
    "alpha = 0.99\n",
    "tau = 1\n",
    "entropy_coef = 0.01\n",
    "value_loss_coef = 0.5\n",
    "log_interval = 10\n",
    "num_processes = 20\n",
    "num_steps = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next part is based on [Ross Wightman's work](https://github.com/rwightman/pytorch-pommerman-rl/blob/master/envs/pommerman.py) on the Pommerman challenge where the observations are one hot encoded and compressed. The features function in the buttom takes the observations as input and returns a $9\\times11\\times11$ obervation matrix and $1\\times3$ feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_FEATURE_CONFIG = {\n",
    "    'recode_agents': True,\n",
    "    'compact_powerups': True,\n",
    "    'compact_structure': True,\n",
    "    'rescale': True,\n",
    "}\n",
    "\n",
    "\n",
    "def make_np_float(feature):\n",
    "    return np.array(feature).astype(np.float32)\n",
    "\n",
    "\n",
    "def _rescale(x):\n",
    "    return (x - 0.5) * 2.0\n",
    "\n",
    "\n",
    "def featurize(obs, agent_id, config):\n",
    "    max_item = pommerman.constants.Item.Agent3.value\n",
    "\n",
    "    ob = obs[\"board\"]\n",
    "    ob_bomb_blast_strength = obs[\"bomb_blast_strength\"].astype(np.float32) / pommerman.constants.AGENT_VIEW_SIZE\n",
    "    ob_bomb_life = obs[\"bomb_life\"].astype(np.float32) / pommerman.constants.DEFAULT_BOMB_LIFE\n",
    "\n",
    "    # one hot encode the board items\n",
    "    ob_values = max_item + 1\n",
    "    ob_hot = np.eye(ob_values)[ob]\n",
    "\n",
    "    # replace agent item channels with friend, enemy, self channels\n",
    "    if config['recode_agents']:\n",
    "        self_value = pommerman.constants.Item.Agent0.value + agent_id\n",
    "        enemies = np.logical_and(ob >= pommerman.constants.Item.Agent0.value, ob != self_value)\n",
    "        self = (ob == self_value)\n",
    "        friends = (ob == pommerman.constants.Item.AgentDummy.value)\n",
    "        ob_hot[:, :, 9] = friends.astype(np.float32)\n",
    "        ob_hot[:, :, 10] = self.astype(np.float32)\n",
    "        ob_hot[:, :, 11] = enemies.astype(np.float32)\n",
    "        ob_hot = np.delete(ob_hot, np.s_[12::], axis=2)\n",
    "\n",
    "    if config['compact_powerups']:\n",
    "        # replace powerups with single channel\n",
    "        powerup = ob_hot[:, :, 6] * 0.5 + ob_hot[:, :, 7] * 0.66667 + ob_hot[:, :, 8]\n",
    "        ob_hot[:, :, 6] = powerup\n",
    "        ob_hot = np.delete(ob_hot, [7, 8], axis=2)\n",
    "\n",
    "    # replace bomb item channel with bomb life\n",
    "    ob_hot[:, :, 3] = ob_bomb_life\n",
    "\n",
    "    if config['compact_structure']:\n",
    "        ob_hot[:, :, 0] = 0.5 * ob_hot[:, :, 0] + ob_hot[:, :, 5]  # passage + fog\n",
    "        ob_hot[:, :, 1] = 0.5 * ob_hot[:, :, 2] + ob_hot[:, :, 1]  # rigid + wood walls\n",
    "        ob_hot = np.delete(ob_hot, [2], axis=2)\n",
    "        # replace former fog channel with bomb blast strength\n",
    "        ob_hot[:, :, 5] = ob_bomb_blast_strength\n",
    "    else:\n",
    "        # insert bomb blast strength next to bomb life\n",
    "        ob_hot = np.insert(ob_hot, 4, ob_bomb_blast_strength, axis=2)\n",
    "\n",
    "    self_ammo = make_np_float([obs[\"ammo\"]])\n",
    "    self_blast_strength = make_np_float([obs[\"blast_strength\"]])\n",
    "    self_can_kick = make_np_float([obs[\"can_kick\"]])\n",
    "\n",
    "    ob_hot = ob_hot.transpose((2, 0, 1))  # PyTorch tensor layout compat\n",
    "\n",
    "    if config['rescale']:\n",
    "        ob_hot = _rescale(ob_hot)\n",
    "        self_ammo = _rescale(self_ammo / 10)\n",
    "        self_blast_strength = _rescale(self_blast_strength / pommerman.constants.AGENT_VIEW_SIZE)\n",
    "        self_can_kick = _rescale(self_can_kick)\n",
    "\n",
    "    return [ob_hot], [np.concatenate([self_ammo, self_blast_strength, self_can_kick])]\n",
    "\n",
    "\n",
    "def features(obs, feature_config=DEFAULT_FEATURE_CONFIG):\n",
    "    obs_im, obs_other = featurize(\n",
    "                obs,\n",
    "                0,\n",
    "                feature_config)\n",
    "    return obs_im, obs_other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A2C is based on two deep neural networks. One working as a critic and the other as the actor. Both networks make an assessment of the current state. The actor uses this to decide which action would be the most favorable and the critic estimates the state value. The state value is an estimate of how good the current state is. When training the actor, the change in state value is used as an indication of how good the action was in the given state. As illustrated in the figure below, the networks share the same architecture before the last layers where they separate. The observations which are described above are one hot encoded and compressed into a 11 $\\times$ 11 $\\times$ 9 matrix $B$ using the function *featurize* from [Ross Wightman's work](https://github.com/rwightman/pytorch-pommerman-rl/blob/master/envs/pommerman.py). The same function also outputs a 1X3 vector $F$ with information about the agent's ammo, blast strength and whether the agent can kick bombs. $B$ is fed to a three-layer convolution neural networks (CNN) with 64 filters each. Batch normalization is performed after every CNN layer. The output of the last CNN is fed to a dense neural network (DNN) with 1024 nodes which then again is fed to a DNN with 512 nodes. *F* is fed to DNN with 3 nodes and the output is concatenated with the output from the DNN with 512 nodes into a vector of length 515. This vector is then fed to a layer of Gated Recurrent Units (GRU) in order to consider previous states when assessing the current state. The output of the GRU is used as an input in two different DNN which corresponds to the actor and the critic. Rectified Linear Units (ReLU) is used as activation function on the output of every layer in the architecture, except for the critic and actor layer where \\textit{tanh} and \\textit{softmax} is used respectively. \\textit{Tanh} is used on the critic because the state value should be negative when the current state is estimated to yield a negative reward in the future. The softmax is used on the output of the actor since the outputs of the actor are treated as the probability of each action being chosen during training.\n",
    "![Network architecture](Images/Model.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor_Critic(nn.Module):\n",
    "    \"\"\"Actor and critic - networks\"\"\"\n",
    "\n",
    "    def __init__(self, n_inputs, n_outputs, inputs_other, n_conv_output):\n",
    "        super(Actor_Critic, self).__init__()\n",
    "        # network\n",
    "        self.CNN = nn.Sequential(\n",
    "                    nn.Conv2d(9, 64, 3, stride=1, padding=1),\n",
    "                    nn.BatchNorm2d(64),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv2d(64, 64, 3, stride=1, padding=1),\n",
    "                    nn.BatchNorm2d(64),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv2d(64, 64, 3, stride=1, padding=1),\n",
    "                    nn.BatchNorm2d(64),\n",
    "                    nn.ReLU(),\n",
    "                    )\n",
    "        self.CNN_mlp = nn.Sequential(        \n",
    "                    nn.Linear(n_conv_output, 1024, bias=True),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(1024, 512, bias=True),\n",
    "                    nn.ReLU(),\n",
    "                    )\n",
    "                    \n",
    "        self.fnn_other = nn.Sequential(\n",
    "                                    nn.Linear(inputs_other, inputs_other, bias=True),\n",
    "                                    nn.ReLU(),\n",
    "                                    )\n",
    "        self.actor = nn.Sequential(\n",
    "                                    nn.Linear(515, 6, bias=False),\n",
    "                                    )\n",
    "        self.state_value = nn.Sequential(\n",
    "                                    nn.Linear(515, 1, bias=False),\n",
    "                                    nn.Tanh(),\n",
    "                                    )\n",
    "        self.GRU = nn.GRUCell(n_inputs, 515)\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.entropies = []\n",
    "        self.log_prob = []\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return get_variable(torch.zeros(1, 515))\n",
    "\n",
    "    def forward(self, x_im, x_other, hxs, batch_size=1):\n",
    "        out = []\n",
    "        x = self.CNN(x_im)\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = self.CNN_mlp(x)\n",
    "        out.append(x)\n",
    "        if batch_size > 1:\n",
    "            x_other = self.input_norm(x_other)\n",
    "        x = self.fnn_other(x_other)\n",
    "        out.append(x)\n",
    "        out = torch.cat(out, dim=1)\n",
    "        out = hxs = self.GRU(out, hxs)\n",
    "        action_scores = self.actor(out)\n",
    "        state_values = self.state_value(out)\n",
    "        return action_scores, state_values[0], hxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PommermanAgent(BaseAgent):\n",
    "    # This is the agent class compitable with the pommerman enviroment.\n",
    "    def __init__(self, character=characters.Bomber, mode='old', model=\"saved_models/policy_network_power_up3\"):\n",
    "        super(PommermanAgent, self).__init__(character)\n",
    "\n",
    "        n_inputs = 515\n",
    "        n_conv_output = 7744\n",
    "        inputs_other = 3\n",
    "        n_outputs = 6\n",
    "        \n",
    "        self.net = Actor_Critic(n_inputs, n_outputs, inputs_other, n_conv_output)\n",
    "        \n",
    "        if use_cuda:\n",
    "            self.net = self.net.cuda()\n",
    "            if mode is not \"new\":\n",
    "                self.net.load_state_dict(torch.load(model))\n",
    "        elif mode is not \"new\":\n",
    "                self.net.load_state_dict(torch.load(model, map_location='cpu'))\n",
    "\n",
    "        self.hxs = self.net.init_hidden()\n",
    "\n",
    "    def act(self, obs, action_space=None):\n",
    "        obs_im, obs_other = features(obs)\n",
    "        self.net.eval()\n",
    "        with torch.no_grad():\n",
    "            action_scores, _, hxs = self.net(get_variable(torch.Tensor(obs_im)), get_variable(torch.Tensor(obs_other)), self.hxs)\n",
    "            self.hxs = hxs\n",
    "        return action_scores.argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of agents (exactly four)\n",
    "agent_list = [\n",
    "    PommermanAgent(model=\"saved_models/policy_network_team_agent0\"),\n",
    "    agents.SimpleAgent(),\n",
    "    PommermanAgent(model=\"saved_models/policy_network_team_agent2\"),\n",
    "    agents.SimpleAgent(),\n",
    "]\n",
    "# Make the \"Team\" environment using the agent list\n",
    "env = pommerman.make('PommeTeamCompetitionFast-v0', agent_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training, the environment is first initialized, and the observation is fed to the neural network.The critic then outputs the state value, while the actor outputs probability of every action. Using the probabilities from the actor, an action is sampled. In other words, if the actor outputs that there is a 26% probability of the agent laying a bomb, this action will be sampled 26% of the times. The action is then performed in the environment and a new state and reward are returned from the environment. This corresponds to one step. After every step, the reward, state value, log likelihoods of the actions and entropy are saved in the model. After 20 steps, the saved values are used to calculate the gradients used to update the critic and actor. First, the discounted return $R_t$  is calculated and then used to calculate the value loss $L_v$ using \n",
    "$\\frac{1}{2}\\sum_t||\\hat{V}_\\phi^\\pi(s)_t-R_t||$. Following, the Advantage $\\hat{A}^\\pi(s_t, a_t)$ is calculated using $r(s_t, a_t) + \\gamma\\hat{V}_\\theta^\\pi(s_t')- \\hat{V}_\\theta^\\pi(s_t)$\n",
    "The advantage is used to give the agent a better understanding of what defines a good action. If the advantage was not included, all actions would be punished if the agent was in a state with only negative outcomes. However, when advantage is included, an action is regarded as good if the state value after the action is better than before. The policy loss $L_p$ is calculated using $\\nabla_\\theta J(\\theta) \\approx \\nabla_\\theta \\log{\\pi_\\theta(a_t|s_t)\\hat{A}^\\pi(s_t,a_t)}-\\beta*H(\\pi)$ The entropy $H(\\pi)$ is subtracted from the policy loss to encourage exploration by restricting an action to have a too high probability compared to all other actions. Entropy describes the spread of the action distribution. The losses are used to obtain the gradient with respect to the network parameters. Each of these gradients is clipped to prevent overly-large parameter updates which can destabilize the policy. Root Mean Square Propagation Algorithm (RMSprop) was used as optimizer. The pseudocode for updating the parameter updates of the actor-critic algorithm is: \n",
    "\n",
    "* take N timesteps t $a_t\\sim\\pi_\\theta(a_t|s_t)$, get $(s_t, a_t, s'_t, r_t)$\n",
    "\n",
    "\n",
    "\n",
    "* evaluate return $R_t = \\sum\\limits_{t}\\gamma\\cdot r(s_t, a_t)$\n",
    "\n",
    "* evaluate value loss $L_v =\\frac{1}{2}\\sum_t||\\hat{V}_\\phi^\\pi(s)_t-R_t||$\n",
    "\n",
    "\n",
    "* evaluate advantage $\\hat{A}^\\pi(s_t, a_t) = r(s_t, a_t) + \\gamma\\hat{V}_\\theta^\\pi(s_t')- \\hat{V}_\\theta^\\pi(s_t)$\n",
    "\n",
    "\n",
    "\n",
    "* evaluate policy loss $L_p= -\\nabla_\\theta \\log{\\pi_\\theta(a_t|s_t)\\hat{A}^\\pi(s_t,a_t)}-\\beta*H(\\pi)$\n",
    "\n",
    "\n",
    "\n",
    "* calculate gradient $\\nabla_\\theta J(\\theta) \\approx  L_p + 0.5 \\cdot L_v$\n",
    "\n",
    "\n",
    "\n",
    "* update parameters of the network $\\theta \\leftarrow \\theta+\\gamma\\nabla_\\theta J(\\theta)$\n",
    "\n",
    "\n",
    "\n",
    "$s_t$ is the state of the timestep t, $a_t$ is the action of the timestep t, $\\pi$ is the policy of the actor, $\\theta$ are the parameters of the actor, $\\hat{V}^\\pi\\phi$ is sample state value, $\\phi$ are the parameters of the critic, $H$ is the entropy of the actions, $\\beta$ is the entropy coefficient and $\\gamma$ is the discount factor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, hxs, agent, model):\n",
    "    obs_im, obs_other = features(state[agent])\n",
    "    state_im = get_variable(torch.from_numpy(np.array(obs_im)).float())\n",
    "    state_other = get_variable(torch.from_numpy(np.array(obs_other)).float())\n",
    "    action_scores, state_value, hxs = model(state_im, state_other, hxs)\n",
    "    probs = F.softmax(action_scores, dim=-1)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample().detach()\n",
    "    log_prob = m.log_prob(action)\n",
    "    entropy = m.entropy()\n",
    "\n",
    "    model.log_prob.append(log_prob)\n",
    "    model.values.append(state_value)\n",
    "    model.entropies.append(entropy)\n",
    "\n",
    "    return action.item(), hxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_episode(done, state, hxs, agent, model, optimizer):\n",
    "    entropies = model.entropies\n",
    "    values = model.values\n",
    "    rewards = model.rewards\n",
    "    retain_graph = False\n",
    "    log_probs = model.log_prob\n",
    "    R = get_variable(torch.zeros(1, 1))\n",
    "    if not done:\n",
    "        retain_graph = True\n",
    "        obs_im, obs_other = features(state[agent])\n",
    "        state_im = get_variable(torch.from_numpy(np.array(obs_im)).float())\n",
    "        state_other = get_variable(torch.from_numpy(np.array(obs_other)).float())\n",
    "        _, state_value, hxs = model(state_im, state_other, hxs)\n",
    "        R = state_value.detach()\n",
    "\n",
    "    values.append(R)\n",
    "    policy_loss = 0\n",
    "    value_loss = 0\n",
    "    gae = get_variable(torch.zeros(1, 1))\n",
    "\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        R = gamma * R + rewards[i]\n",
    "        if R > 1:\n",
    "            R = 1\n",
    "        error = R - values[i]\n",
    "        value_loss = value_loss + 0.5*error.pow(2)\n",
    "\n",
    "        # Generalized Advantage Estimataion\n",
    "        delta_t = rewards[i] + gamma * \\\n",
    "            values[i + 1] - values[i]\n",
    "        gae = gae * gamma * tau + delta_t\n",
    "\n",
    "        policy_loss = policy_loss - \\\n",
    "            log_probs[i] * gae.detach() - entropy_coef * entropies[i]\n",
    "    optimizer.zero_grad()\n",
    "    (policy_loss + value_loss_coef * value_loss).backward(retain_graph=retain_graph)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "\n",
    "    optimizer.step()\n",
    "    del model.rewards[:]\n",
    "    del model.entropies[:]\n",
    "    del model.values[:]\n",
    "    del model.log_prob[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model was trained with agents in the top left and bottom right corners. Separate sets of networks were trained for the agent in either corner. To encourage cooperation and a more aggressive style of play, different rewards described in the table below are used.\n",
    " \n",
    "\n",
    "\n",
    " Scenario       | Reward\n",
    "----------------|--------------\n",
    "Lays a bomb     |       0.007\n",
    "Gets a powerup  |       0.8\n",
    "Wins the game   |      1\n",
    "Draws the game  |      -1\n",
    "       Dies     |      -1\n",
    "Teammate Dies   |      -0.5\n",
    "Teammate visible|      0.008\n",
    "\n",
    "Due to GPU memory restraints, only one agent is trained for each game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    win = []\n",
    "    eps = np.finfo(np.float32).eps.item()\n",
    "    number_of_bombs = 0\n",
    "    running_reward = 10\n",
    "    i_episode = 0\n",
    "    seen_positions = []\n",
    "    power_ups = 0\n",
    "    agent = 2\n",
    "    team_agent = 0\n",
    "    for _ in range(200):\n",
    "        if agent == 2:\n",
    "            agent = 0\n",
    "            team_agent = 2\n",
    "        else:\n",
    "            agent = 2\n",
    "            team_agent = 0\n",
    "        model = PommermanAgent(mode=\"test\", model=\"saved_models/policy_network_team_agent\"+str(agent)).net\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=lr, eps=eps, alpha=alpha)\n",
    "        model.train()\n",
    "        for _ in range(1000):\n",
    "            total_steps = 0\n",
    "            state = env.reset()\n",
    "            old_blast_strength = state[agent]['blast_strength']\n",
    "            old_ammo = state[agent]['ammo']\n",
    "            old_can_kick = state[agent]['can_kick']\n",
    "            ammo = 1\n",
    "            hxs = model.init_hidden()\n",
    "            model.zero_grad()\n",
    "            alive0 = 1\n",
    "            alive2 = 1\n",
    "            for t in range(10000):  # Don't infinite loop while learning\n",
    "                for steps in range(num_steps):\n",
    "                    total_steps += 1\n",
    "                    actions = env.act(state)\n",
    "                    actions[agent], hxs = select_action(state, hxs, agent, model)\n",
    "                    state, reward, done, _ = env.step(actions)\n",
    "                    # region rewardfunction\n",
    "                    if alive0 and pommerman.constants.Item.Agent0.value not in state[team_agent]['alive']:\n",
    "                        if agent == 0:\n",
    "                            reward[agent] = -1\n",
    "                            done = 1\n",
    "                        elif agent == 2:\n",
    "                            reward[agent] = -0.5\n",
    "                    if alive2 and pommerman.constants.Item.Agent2.value not in state[agent]['alive']:\n",
    "                        if agent == 0:\n",
    "                            reward[agent] = -0.5\n",
    "                        elif agent == 2:\n",
    "                            reward[agent] = -1\n",
    "                            done = 1\n",
    "                    if state[agent]['teammate'].value in state[agent]['board']:\n",
    "                        reward[agent] += 0.008\n",
    "                    if state[agent]['position'] not in seen_positions:\n",
    "                        seen_positions.append(state[agent]['position'])\n",
    "                    if actions[agent] == 5 and reward[agent] != -1 and ammo != 0:\n",
    "                        reward[agent] += 0.007\n",
    "                        number_of_bombs += 1\n",
    "                    if state[agent]['blast_strength'] > old_blast_strength:\n",
    "                        reward[agent] += 0.8\n",
    "                        power_ups += 1\n",
    "                    if state[agent]['ammo'] > old_ammo:\n",
    "                        reward[agent] += 0.8\n",
    "                        power_ups += 1\n",
    "                    if state[agent]['can_kick'] > old_can_kick:\n",
    "                        reward[agent] += 0.8\n",
    "                        power_ups += 1\n",
    "                    ammo = state[agent]['ammo'] != 0\n",
    "                    old_blast_strength = max(state[agent]['blast_strength'], old_blast_strength)\n",
    "                    old_ammo = max(state[agent]['ammo'], old_ammo)\n",
    "                    old_can_kick = max(state[agent]['can_kick'], old_can_kick)\n",
    "                    alive0 = pommerman.constants.Item.Agent0.value in state[team_agent]['alive']\n",
    "                    alive2 = pommerman.constants.Item.Agent2.value in state[agent]['alive']\n",
    "                    # endregion\n",
    "                    model.rewards.append(get_variable(torch.from_numpy(np.array(reward[agent])).float()))\n",
    "                    if done or reward[agent] == -1:\n",
    "                        win.append(reward[agent] >= 1)\n",
    "                        seen_positions = []\n",
    "                        done = 1\n",
    "                        break\n",
    "                finish_episode(done, state, hxs, agent, model, optimizer)\n",
    "                if done: \n",
    "                    break\n",
    "            running_reward = running_reward * 0.99 + total_steps * 0.01\n",
    "            if i_episode % log_interval == 0:\n",
    "                print('Episode {}\\tPower ups per match: {:.2f}\\tAverage length: {:.2f}\\tWin percentage: {:.2f}\\tBombs per match: {:.2f}'.format(\n",
    "                    i_episode, power_ups/log_interval, running_reward, np.mean(win), number_of_bombs/log_interval))\n",
    "                win = []\n",
    "                power_ups = 0\n",
    "                number_of_bombs = 0\n",
    "            i_episode += 1\n",
    "        torch.save(model.state_dict(), \"saved_models/policy_network_team_agent\"+str(agent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, it is possible to run the agents in the enviroment. The agents have been trained for 100 000 matches each. Have in mind that there is a high variance of performance since the agents still are in need of more training. A weakness of the AI agents is their unwillingness to move away from their starting corner. This is most likely due to overfitting and could be solved in numerous ways. One solution could be to assign a random starting position for each game, forcing the agent to accommodate a new environment every game. A consequence of the agent's reluctance to explore during the game is, that they rarely see each other (the agents' vision is limited). This is probably the cause of the lack of cooperation between the two AI agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print all possible environments in the Pommerman registry\n",
    "print(pommerman.REGISTRY)\n",
    "\n",
    "\n",
    "# Create a set of agents (exactly four)\n",
    "agent_list = [\n",
    "PommermanAgent(mode=\"test\", model=\"saved_models/policy_network_team_agent0\"),\n",
    "agents.SimpleAgent(),\n",
    "PommermanAgent(mode=\"test\", model=\"saved_models/policy_network_team_agent2\"),\n",
    "agents.SimpleAgent(),\n",
    "]\n",
    "\n",
    "# Make the Team environment using the agent list\n",
    "\n",
    "env = pommerman.make('PommeTeamCompetition-v0', agent_list)\n",
    "\n",
    "# Run the episodes just like OpenAI Gym\n",
    "\n",
    "state = env.reset()\n",
    "num_no_mov_agent1 = 0\n",
    "num_no_mov_agent3 = 0\n",
    "done = False\n",
    "while not done:\n",
    "    env.render()\n",
    "    actions = env.act(state)\n",
    "    # An unelegant method to stop the agents to get stuck:\n",
    "    old_position_agent1 = state[1]['position']\n",
    "    old_position_agent3 = state[3]['position']\n",
    "    if num_no_mov_agent1 > 20:\n",
    "        actions[1] = random.choice(range(0, 5))\n",
    "    if num_no_mov_agent3 > 20:\n",
    "        actions[3] = random.choice(range(0, 5))\n",
    "    state, reward, done, info = env.step(actions)\n",
    "    if abs(state[1]['position'][0]-old_position_agent1[0])+abs(state[1]['position'][1]-old_position_agent1[1]) == 0:\n",
    "        num_no_mov_agent1 += 1\n",
    "    else:\n",
    "        num_no_mov_agent1 = 0\n",
    "    if abs(state[3]['position'][0]-old_position_agent3[0])+abs(state[3]['position'][1]-old_position_agent3[1]) == 0:\n",
    "        num_no_mov_agent3 += 1\n",
    "    else:\n",
    "        num_no_mov_agent3 = 0\n",
    "\n",
    "print(info)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to train the agents continuing from the model after 100 000 matches of training. The models are automatically saved in the **saved models** folder every 1000 match. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    main()\n",
    "    print('Done')\n",
    "except KeyboardInterrupt:\n",
    "    print('Keyboard interrupt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future implementations\n",
    "\n",
    "One of the biggest challenges we faced in this project was the immense training time required using A2C. Thus, optimization of the training code should be considered in future work. Parallelization of the environment during training and possibly even implementation on a cluster system is an obvious next step. This would greatly reduce the training time.\n",
    "One approach to reducing the overfitting of an agent to its starting corner is to randomize the starting position of each agent. This would potentially force the AI agent to learn how to cope in a different environment every game. This approach could provide rewards for behavior outside of the AI agent's starting area and would increase the likelihood of the AI agent leaving the said area.\n",
    "To further improve the performance of the AI agents, they should be trained against other agents than the simple agents. Training the AI agents against other AI agents could force the against to learn how to cope with different tactic styles. Hopefully, it would help hinder overfitting the AI agents playing style to playing against the simple agents. However, as mentioned before, this was attempted and resulted in very slow training due to the agents not leaving their corner, so this problem should be addressed first.\n",
    "If we had a better computer and more time, we would have implemented the Actor-Critic using Kronecker-Factored Trust-Region (ACKTR) as it has been shown to learn quicker than A2C, especially when using large batch sizes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
