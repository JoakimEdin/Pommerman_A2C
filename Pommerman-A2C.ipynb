{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:\\users\\joakim edin\\documents\\pommerman_a2c\n",
      "Requirement already satisfied, skipping upgrade: docker~=3.1 in c:\\users\\joakim edin\\appdata\\roaming\\python\\python37\\site-packages (from pommerman==0.2.0) (3.5.1)\n",
      "Requirement already satisfied, skipping upgrade: gym~=0.10 in c:\\users\\joakim edin\\appdata\\roaming\\python\\python37\\site-packages (from pommerman==0.2.0) (0.10.8)\n",
      "Requirement already satisfied, skipping upgrade: scipy~=1.0 in c:\\users\\joakim edin\\appdata\\roaming\\python\\python37\\site-packages (from pommerman==0.2.0) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: Pillow~=5.0 in c:\\users\\joakim edin\\appdata\\roaming\\python\\python37\\site-packages (from pommerman==0.2.0) (5.2.0)\n",
      "Requirement already satisfied, skipping upgrade: ruamel.yaml~=0.15 in c:\\users\\joakim edin\\appdata\\roaming\\python\\python37\\site-packages (from pommerman==0.2.0) (0.15.74)\n",
      "Requirement already satisfied, skipping upgrade: Flask~=0.12 in c:\\users\\joakim edin\\appdata\\roaming\\python\\python37\\site-packages (from pommerman==0.2.0) (0.12.4)\n",
      "Requirement already satisfied, skipping upgrade: requests~=2.18 in c:\\users\\joakim edin\\appdata\\roaming\\python\\python37\\site-packages (from pommerman==0.2.0) (2.19.1)\n",
      "Requirement already satisfied, skipping upgrade: jsonmerge~=1.5.1 in c:\\users\\joakim edin\\appdata\\roaming\\python\\python37\\site-packages (from pommerman==0.2.0) (1.5.1)\n",
      "Requirement already satisfied, skipping upgrade: astroid>=2 in c:\\users\\joakim edin\\appdata\\roaming\\python\\python37\\site-packages (from pommerman==0.2.0) (2.0.4)\n",
      "Requirement already satisfied, skipping upgrade: isort~=4.3.4 in c:\\users\\joakim edin\\appdata\\roaming\\python\\python37\\site-packages (from pommerman==0.2.0) (4.3.4)\n",
      "Requirement already satisfied, skipping upgrade: pylint>=2 in c:\\users\\joakim edin\\appdata\\roaming\\python\\python37\\site-packages (from pommerman==0.2.0) (2.1.1)\n",
      "Requirement already satisfied, skipping upgrade: websockets~=6.0 in c:\\users\\joakim edin\\appdata\\roaming\\python\\python37\\site-packages (from pommerman==0.2.0) (6.0)\n",
      "Requirement already satisfied, skipping upgrade: websocket-client~=0.53.0 in c:\\users\\joakim edin\\appdata\\roaming\\python\\python37\\site-packages (from pommerman==0.2.0) (0.53.0)\n",
      "Requirement already satisfied, skipping upgrade: python-cli-ui~=0.7.1 in c:\\users\\joakim edin\\appdata\\roaming\\python\\python37\\site-packages (from pommerman==0.2.0) (0.7.3)\n",
      "Requirement already satisfied, skipping upgrade: python-rapidjson~=0.6.3 in c:\\users\\joakim edin\\appdata\\roaming\\python\\python37\\site-packages (from pommerman==0.2.0) (0.6.3)\n",
      "Requirement already satisfied, skipping upgrade: pypiwin32==223; sys_platform == \"win32\" and python_version >= \"3.6\" in c:\\users\\joakim edin\\appdata\\roaming\\python\\python37\\site-packages (from docker~=3.1->pommerman==0.2.0) (223)\n",
      "Requirement already satisfied, skipping upgrade: docker-pycreds>=0.3.0 in c:\\users\\joakim edin\\appdata\\roaming\\python\\python37\\site-packages (from docker~=3.1->pommerman==0.2.0) (0.3.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.4.0 in c:\\users\\joakim edin\\appdata\\roaming\\python\\python37\\site-packages (from docker~=3.1->pommerman==0.2.0) (1.11.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.10.4 in c:\\users\\joakim edin\\appdata\\roaming\\python\\python37\\site-packages (from gym~=0.10->pommerman==0.2.0) (1.15.1)\n",
      "Requirement already satisfied, skipping upgrade: pyglet>=1.2.0 in c:\\users\\joakim edin\\appdata\\roaming\\python\\python37\\site-packages (from gym~=0.10->pommerman==0.2.0) (1.3.2)\n",
      "Requirement already satisfied, skipping upgrade: click>=2.0 in c:\\users\\joakim edin\\appdata\\roaming\\python\\python37\\site-packages (from Flask~=0.12->pommerman==0.2.0) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: Jinja2>=2.4 in c:\\users\\joakim edin\\appdata\\roaming\\python\\python37\\site-packages (from Flask~=0.12->pommerman==0.2.0) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: Werkzeug>=0.7 in c:\\users\\joakim edin\\appdata\\roaming\\python\\python37\\site-packages (from Flask~=0.12->pommerman==0.2.0) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: itsdangerous>=0.21 in c:\\users\\joakim edin\\appdata\\roaming\\python\\python37\\site-packages (from Flask~=0.12->pommerman==0.2.0) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.24,>=1.21.1 in c:\\users\\joakim edin\\appdata\\roaming\\python\\python37\\site-packages (from requests~=2.18->pommerman==0.2.0) (1.23)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in c:\\users\\joakim edin\\appdata\\roaming\\python\\python37\\site-packages (from requests~=2.18->pommerman==0.2.0) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.8,>=2.5 in c:\\users\\joakim edin\\appdata\\roaming\\python\\python37\\site-packages (from requests~=2.18->pommerman==0.2.0) (2.7)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\users\\joakim edin\\appdata\\roaming\\python\\python37\\site-packages (from requests~=2.18->pommerman==0.2.0) (2018.8.24)\n",
      "Requirement already satisfied, skipping upgrade: jsonschema in c:\\users\\joakim edin\\appdata\\roaming\\python\\python37\\site-packages (from jsonmerge~=1.5.1->pommerman==0.2.0) (2.6.0)\n",
      "Requirement already satisfied, skipping upgrade: wrapt in c:\\users\\joakim edin\\appdata\\roaming\\python\\python37\\site-packages (from astroid>=2->pommerman==0.2.0) (1.10.11)\n",
      "Requirement already satisfied, skipping upgrade: lazy-object-proxy in c:\\users\\joakim edin\\appdata\\roaming\\python\\python37\\site-packages (from astroid>=2->pommerman==0.2.0) (1.3.1)\n",
      "Requirement already satisfied, skipping upgrade: colorama; sys_platform == \"win32\" in c:\\users\\joakim edin\\appdata\\roaming\\python\\python37\\site-packages (from pylint>=2->pommerman==0.2.0) (0.3.9)\n",
      "Requirement already satisfied, skipping upgrade: mccabe in c:\\users\\joakim edin\\appdata\\roaming\\python\\python37\\site-packages (from pylint>=2->pommerman==0.2.0) (0.6.1)\n",
      "Requirement already satisfied, skipping upgrade: tabulate in c:\\users\\joakim edin\\appdata\\roaming\\python\\python37\\site-packages (from python-cli-ui~=0.7.1->pommerman==0.2.0) (0.8.2)\n",
      "Requirement already satisfied, skipping upgrade: unidecode in c:\\users\\joakim edin\\appdata\\roaming\\python\\python37\\site-packages (from python-cli-ui~=0.7.1->pommerman==0.2.0) (1.0.22)\n",
      "Requirement already satisfied, skipping upgrade: pywin32>=223 in c:\\users\\joakim edin\\appdata\\roaming\\python\\python37\\site-packages (from pypiwin32==223; sys_platform == \"win32\" and python_version >= \"3.6\"->docker~=3.1->pommerman==0.2.0) (224)\n",
      "Requirement already satisfied, skipping upgrade: future in c:\\users\\joakim edin\\appdata\\roaming\\python\\python37\\site-packages (from pyglet>=1.2.0->gym~=0.10->pommerman==0.2.0) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in c:\\users\\joakim edin\\appdata\\roaming\\python\\python37\\site-packages (from Jinja2>=2.4->Flask~=0.12->pommerman==0.2.0) (1.0)\n",
      "Installing collected packages: pommerman\n",
      "  Found existing installation: pommerman 0.2.0\n",
      "    Uninstalling pommerman-0.2.0:\n",
      "      Successfully uninstalled pommerman-0.2.0\n",
      "  Running setup.py install for pommerman: started\n",
      "    Running setup.py install for pommerman: finished with status 'done'\n",
      "Successfully installed pommerman-0.2.0\n"
     ]
    }
   ],
   "source": [
    "# Run this after a change in the Pommerman folder\n",
    "!py -m pip install -U . --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pommerman\n",
    "from pommerman import agents\n",
    "from pommerman.agents import BaseAgent\n",
    "from pommerman import characters\n",
    "import pommerman.envs\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "def get_variable(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "gamma = 0.99\n",
    "lr = 2.5e-4\n",
    "eps = 1e-5\n",
    "alpha = 0.99\n",
    "tau = 1\n",
    "entropy_coef = 0.01\n",
    "value_loss_coef = 0.5\n",
    "log_interval = 10\n",
    "num_processes = 20\n",
    "num_steps = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next part is based on [Ross Wightman's work](https://github.com/rwightman/pytorch-pommerman-rl/blob/master/envs/pommerman.py) on the Pommerman challenge where the observations are one hot encoded and compressed. The features function in the buttom takes the observations as input and returns a $9\\times11\\times11$ obervation matrix and $1\\times3$ feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_FEATURE_CONFIG = {\n",
    "    'recode_agents': True,\n",
    "    'compact_powerups': True,\n",
    "    'compact_structure': True,\n",
    "    'rescale': True,\n",
    "}\n",
    "\n",
    "\n",
    "def make_np_float(feature):\n",
    "    return np.array(feature).astype(np.float32)\n",
    "\n",
    "\n",
    "def _rescale(x):\n",
    "    return (x - 0.5) * 2.0\n",
    "\n",
    "\n",
    "def featurize(obs, agent_id, config):\n",
    "    max_item = pommerman.constants.Item.Agent3.value\n",
    "\n",
    "    ob = obs[\"board\"]\n",
    "    ob_bomb_blast_strength = obs[\"bomb_blast_strength\"].astype(np.float32) / pommerman.constants.AGENT_VIEW_SIZE\n",
    "    ob_bomb_life = obs[\"bomb_life\"].astype(np.float32) / pommerman.constants.DEFAULT_BOMB_LIFE\n",
    "\n",
    "    # one hot encode the board items\n",
    "    ob_values = max_item + 1\n",
    "    ob_hot = np.eye(ob_values)[ob]\n",
    "\n",
    "    # replace agent item channels with friend, enemy, self channels\n",
    "    if config['recode_agents']:\n",
    "        self_value = pommerman.constants.Item.Agent0.value + agent_id\n",
    "        enemies = np.logical_and(ob >= pommerman.constants.Item.Agent0.value, ob != self_value)\n",
    "        self = (ob == self_value)\n",
    "        friends = (ob == pommerman.constants.Item.AgentDummy.value)\n",
    "        ob_hot[:, :, 9] = friends.astype(np.float32)\n",
    "        ob_hot[:, :, 10] = self.astype(np.float32)\n",
    "        ob_hot[:, :, 11] = enemies.astype(np.float32)\n",
    "        ob_hot = np.delete(ob_hot, np.s_[12::], axis=2)\n",
    "\n",
    "    if config['compact_powerups']:\n",
    "        # replace powerups with single channel\n",
    "        powerup = ob_hot[:, :, 6] * 0.5 + ob_hot[:, :, 7] * 0.66667 + ob_hot[:, :, 8]\n",
    "        ob_hot[:, :, 6] = powerup\n",
    "        ob_hot = np.delete(ob_hot, [7, 8], axis=2)\n",
    "\n",
    "    # replace bomb item channel with bomb life\n",
    "    ob_hot[:, :, 3] = ob_bomb_life\n",
    "\n",
    "    if config['compact_structure']:\n",
    "        ob_hot[:, :, 0] = 0.5 * ob_hot[:, :, 0] + ob_hot[:, :, 5]  # passage + fog\n",
    "        ob_hot[:, :, 1] = 0.5 * ob_hot[:, :, 2] + ob_hot[:, :, 1]  # rigid + wood walls\n",
    "        ob_hot = np.delete(ob_hot, [2], axis=2)\n",
    "        # replace former fog channel with bomb blast strength\n",
    "        ob_hot[:, :, 5] = ob_bomb_blast_strength\n",
    "    else:\n",
    "        # insert bomb blast strength next to bomb life\n",
    "        ob_hot = np.insert(ob_hot, 4, ob_bomb_blast_strength, axis=2)\n",
    "\n",
    "    self_ammo = make_np_float([obs[\"ammo\"]])\n",
    "    self_blast_strength = make_np_float([obs[\"blast_strength\"]])\n",
    "    self_can_kick = make_np_float([obs[\"can_kick\"]])\n",
    "\n",
    "    ob_hot = ob_hot.transpose((2, 0, 1))  # PyTorch tensor layout compat\n",
    "\n",
    "    if config['rescale']:\n",
    "        ob_hot = _rescale(ob_hot)\n",
    "        self_ammo = _rescale(self_ammo / 10)\n",
    "        self_blast_strength = _rescale(self_blast_strength / pommerman.constants.AGENT_VIEW_SIZE)\n",
    "        self_can_kick = _rescale(self_can_kick)\n",
    "\n",
    "    return [ob_hot], [np.concatenate([self_ammo, self_blast_strength, self_can_kick])]\n",
    "\n",
    "\n",
    "def features(obs, feature_config=DEFAULT_FEATURE_CONFIG):\n",
    "    obs_im, obs_other = featurize(\n",
    "                obs,\n",
    "                0,\n",
    "                feature_config)\n",
    "    return obs_im, obs_other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Network architecture](Images/Model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor_Critic(nn.Module):\n",
    "    \"\"\"Actor and critic - networks\"\"\"\n",
    "\n",
    "    def __init__(self, n_inputs, n_outputs, inputs_other, n_conv_output):\n",
    "        super(Actor_Critic, self).__init__()\n",
    "        # network\n",
    "        self.CNN = nn.Sequential(\n",
    "                    nn.Conv2d(9, 64, 3, stride=1, padding=1),\n",
    "                    nn.BatchNorm2d(64),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv2d(64, 64, 3, stride=1, padding=1),\n",
    "                    nn.BatchNorm2d(64),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv2d(64, 64, 3, stride=1, padding=1),\n",
    "                    nn.BatchNorm2d(64),\n",
    "                    nn.ReLU(),\n",
    "                    )\n",
    "        self.CNN_mlp = nn.Sequential(        \n",
    "                    nn.Linear(n_conv_output, 1024, bias=True),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(1024, 512, bias=True),\n",
    "                    nn.ReLU(),\n",
    "                    )\n",
    "                    \n",
    "        self.fnn_other = nn.Sequential(\n",
    "                                    nn.Linear(inputs_other, inputs_other, bias=True),\n",
    "                                    nn.ReLU(),\n",
    "                                    )\n",
    "        self.actor = nn.Sequential(\n",
    "                                    nn.Linear(515, 6, bias=False),\n",
    "                                    )\n",
    "        self.state_value = nn.Sequential(\n",
    "                                    nn.Linear(515, 1, bias=False),\n",
    "                                    nn.Tanh(),\n",
    "                                    )\n",
    "        self.GRU = nn.GRUCell(n_inputs, 515)\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.entropies = []\n",
    "        self.log_prob = []\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return get_variable(torch.zeros(1, 515))\n",
    "\n",
    "    def forward(self, x_im, x_other, hxs, batch_size=1):\n",
    "        out = []\n",
    "        x = self.CNN(x_im)\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = self.CNN_mlp(x)\n",
    "        out.append(x)\n",
    "        if batch_size > 1:\n",
    "            x_other = self.input_norm(x_other)\n",
    "        x = self.fnn_other(x_other)\n",
    "        out.append(x)\n",
    "        out = torch.cat(out, dim=1)\n",
    "        out = hxs = self.GRU(out, hxs)\n",
    "        action_scores = self.actor(out)\n",
    "        state_values = self.state_value(out)\n",
    "        return action_scores, state_values[0], hxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PommermanAgent(BaseAgent):\n",
    "    def __init__(self, character=characters.Bomber, mode='old', model=\"saved_models/policy_network_power_up3\"):\n",
    "        super(PommermanAgent, self).__init__(character)\n",
    "\n",
    "        n_inputs = 515\n",
    "        n_conv_output = 7744\n",
    "        inputs_other = 3\n",
    "        n_outputs = 6\n",
    "        \n",
    "        self.net = Actor_Critic(n_inputs, n_outputs, inputs_other, n_conv_output)\n",
    "        \n",
    "        if use_cuda:\n",
    "            self.net = self.net.cuda()\n",
    "            if mode is not \"new\":\n",
    "                self.net.load_state_dict(torch.load(model))\n",
    "        elif mode is not \"new\":\n",
    "                self.net.load_state_dict(torch.load(model, map_location='cpu'))\n",
    "\n",
    "        self.hxs = self.net.init_hidden()\n",
    "\n",
    "    def act(self, obs, action_space=None):\n",
    "        obs_im, obs_other = features(obs)\n",
    "        self.net.eval()\n",
    "        with torch.no_grad():\n",
    "            action_scores, _, hxs = self.net(get_variable(torch.Tensor(obs_im)), get_variable(torch.Tensor(obs_other)), self.hxs)\n",
    "            self.hxs = hxs\n",
    "        return action_scores.argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of agents (exactly four)\n",
    "agent_list = [\n",
    "    PommermanAgent(model=\"saved_models/policy_network_team_agent0\"),\n",
    "    agents.SimpleAgent(),\n",
    "    PommermanAgent(model=\"saved_models/policy_network_team_agent2\"),\n",
    "    agents.SimpleAgent(),\n",
    "]\n",
    "# Make the \"Team\" environment using the agent list\n",
    "env = pommerman.make('PommeTeamCompetitionFast-v0', agent_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, hxs, agent, model):\n",
    "    obs_im, obs_other = features(state[agent])\n",
    "    state_im = get_variable(torch.from_numpy(np.array(obs_im)).float())\n",
    "    state_other = get_variable(torch.from_numpy(np.array(obs_other)).float())\n",
    "    action_scores, state_value, hxs = model(state_im, state_other, hxs)\n",
    "    probs = F.softmax(action_scores, dim=-1)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample().detach()\n",
    "    log_prob = m.log_prob(action)\n",
    "    entropy = m.entropy()\n",
    "\n",
    "    model.log_prob.append(log_prob)\n",
    "    model.values.append(state_value)\n",
    "    model.entropies.append(entropy)\n",
    "\n",
    "    return action.item(), hxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_episode(done, state, hxs, agent, model, optimizer):\n",
    "    entropies = model.entropies\n",
    "    values = model.values\n",
    "    rewards = model.rewards\n",
    "    retain_graph = False\n",
    "    log_probs = model.log_prob\n",
    "    R = get_variable(torch.zeros(1, 1))\n",
    "    if not done:\n",
    "        retain_graph = True\n",
    "        obs_im, obs_other = features(state[agent])\n",
    "        state_im = get_variable(torch.from_numpy(np.array(obs_im)).float())\n",
    "        state_other = get_variable(torch.from_numpy(np.array(obs_other)).float())\n",
    "        _, state_value, hxs = model(state_im, state_other, hxs)\n",
    "        R = state_value.detach()\n",
    "\n",
    "    values.append(R)\n",
    "    policy_loss = 0\n",
    "    value_loss = 0\n",
    "    gae = get_variable(torch.zeros(1, 1))\n",
    "\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        R = gamma * R + rewards[i]\n",
    "        if R > 1:\n",
    "            R = 1\n",
    "        error = R - values[i]\n",
    "        value_loss = value_loss + 0.5*error.pow(2)\n",
    "\n",
    "        # Generalized Advantage Estimataion\n",
    "        delta_t = rewards[i] + gamma * \\\n",
    "            values[i + 1] - values[i]\n",
    "        gae = gae * gamma * tau + delta_t\n",
    "\n",
    "        policy_loss = policy_loss - \\\n",
    "            log_probs[i] * gae.detach() - entropy_coef * entropies[i]\n",
    "    optimizer.zero_grad()\n",
    "    (policy_loss + value_loss_coef * value_loss).backward(retain_graph=retain_graph)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "\n",
    "    optimizer.step()\n",
    "    del model.rewards[:]\n",
    "    del model.entropies[:]\n",
    "    del model.values[:]\n",
    "    del model.log_prob[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    win = []\n",
    "    eps = np.finfo(np.float32).eps.item()\n",
    "    number_of_bombs = 0\n",
    "    running_reward = 10\n",
    "    i_episode = 0\n",
    "    seen_positions = []\n",
    "    power_ups = 0\n",
    "    agent = 2\n",
    "    team_agent = 0\n",
    "    for _ in range(200):\n",
    "        if agent == 2:\n",
    "            agent = 0\n",
    "            team_agent = 2\n",
    "        else:\n",
    "            agent = 2\n",
    "            team_agent = 0\n",
    "        model = PommermanAgent(mode=\"test\", model=\"saved_models/policy_network_team_agent\"+str(agent)).net\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=lr, eps=eps, alpha=alpha)\n",
    "        model.train()\n",
    "        for _ in range(1000):\n",
    "            total_steps = 0\n",
    "            state = env.reset()\n",
    "            old_blast_strength = state[agent]['blast_strength']\n",
    "            old_ammo = state[agent]['ammo']\n",
    "            old_can_kick = state[agent]['can_kick']\n",
    "            ammo = 1\n",
    "            hxs = model.init_hidden()\n",
    "            model.zero_grad()\n",
    "            alive0 = 1\n",
    "            alive2 = 1\n",
    "            for t in range(10000):  # Don't infinite loop while learning\n",
    "                for steps in range(num_steps):\n",
    "                    total_steps += 1\n",
    "                    actions = env.act(state)\n",
    "                    actions[agent], hxs = select_action(state, hxs, agent, model)\n",
    "                    state, reward, done, _ = env.step(actions)\n",
    "                    # region rewardfunction\n",
    "                    if alive0 and pommerman.constants.Item.Agent0.value not in state[team_agent]['alive']:\n",
    "                        if agent == 0:\n",
    "                            reward[agent] = -1\n",
    "                            done = 1\n",
    "                        elif agent == 2:\n",
    "                            reward[agent] = -0.5\n",
    "                    if alive2 and pommerman.constants.Item.Agent2.value not in state[agent]['alive']:\n",
    "                        if agent == 0:\n",
    "                            reward[agent] = -0.5\n",
    "                        elif agent == 2:\n",
    "                            reward[agent] = -1\n",
    "                            done = 1\n",
    "                    if state[agent]['teammate'].value in state[agent]['board']:\n",
    "                        reward[agent] += 0.008\n",
    "                    if state[agent]['position'] not in seen_positions:\n",
    "                        seen_positions.append(state[agent]['position'])\n",
    "                    if actions[agent] == 5 and reward[agent] != -1 and ammo != 0:\n",
    "                        reward[agent] += 0.007\n",
    "                        number_of_bombs += 1\n",
    "                    if state[agent]['blast_strength'] > old_blast_strength:\n",
    "                        reward[agent] += 0.8\n",
    "                        power_ups += 1\n",
    "                    if state[agent]['ammo'] > old_ammo:\n",
    "                        reward[agent] += 0.8\n",
    "                        power_ups += 1\n",
    "                    if state[agent]['can_kick'] > old_can_kick:\n",
    "                        reward[agent] += 0.8\n",
    "                        power_ups += 1\n",
    "                    ammo = state[agent]['ammo'] != 0\n",
    "                    old_blast_strength = max(state[agent]['blast_strength'], old_blast_strength)\n",
    "                    old_ammo = max(state[agent]['ammo'], old_ammo)\n",
    "                    old_can_kick = max(state[agent]['can_kick'], old_can_kick)\n",
    "                    alive0 = pommerman.constants.Item.Agent0.value in state[team_agent]['alive']\n",
    "                    alive2 = pommerman.constants.Item.Agent2.value in state[agent]['alive']\n",
    "                    # endregion\n",
    "                    model.rewards.append(get_variable(torch.from_numpy(np.array(reward[agent])).float()))\n",
    "                    if done or reward[agent] == -1:\n",
    "                        win.append(reward[agent] >= 1)\n",
    "                        seen_positions = []\n",
    "                        done = 1\n",
    "                        break\n",
    "                finish_episode(done, state, hxs, agent, model, optimizer)\n",
    "                if done: \n",
    "                    break\n",
    "            running_reward = running_reward * 0.99 + total_steps * 0.01\n",
    "            if i_episode % log_interval == 0:\n",
    "                print('Episode {}\\tPower ups per match: {:.2f}\\tAverage length: {:.2f}\\tWin percentage: {:.2f}\\tBombs per match: {:.2f}'.format(\n",
    "                    i_episode, power_ups/log_interval, running_reward, np.mean(win), number_of_bombs/log_interval))\n",
    "                win = []\n",
    "                power_ups = 0\n",
    "                number_of_bombs = 0\n",
    "            i_episode += 1\n",
    "        torch.save(model.state_dict(), \"saved_models/policy_network_team_agent\"+str(agent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PommeFFACompetition-v0', 'PommeFFACompetitionFast-v0', 'PommeFFAFast-v0', 'PommeFFA-v1', 'PommeRadio-v2', 'PommeTeamCompetition-v0', 'PommeTeamCompetitionFast-v0', 'PommeTeamCompetition-v1', 'PommeTeam-v0', 'PommeTeamFast-v0']\n",
      "{'result': <Result.Win: 0>, 'winners': [1, 3]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print all possible environments in the Pommerman registry\n",
    "print(pommerman.REGISTRY)\n",
    "\n",
    "\n",
    "# Create a set of agents (exactly four)\n",
    "agent_list = [\n",
    "PommermanAgent(mode=\"test\", model=\"saved_models/policy_network_team_agent0\"),\n",
    "agents.SimpleAgent(),\n",
    "PommermanAgent(mode=\"test\", model=\"saved_models/policy_network_team_agent2\"),\n",
    "agents.SimpleAgent(),\n",
    "]\n",
    "\n",
    "# Make the Team environment using the agent list\n",
    "\n",
    "env = pommerman.make('PommeTeamCompetition-v0', agent_list)\n",
    "\n",
    "# Run the episodes just like OpenAI Gym\n",
    "\n",
    "state = env.reset()\n",
    "num_no_mov_agent1 = 0\n",
    "num_no_mov_agent3 = 0\n",
    "done = False\n",
    "while not done:\n",
    "    env.render()\n",
    "    actions = env.act(state)\n",
    "    # An unelegant method to stop the agents to get stuck:\n",
    "    old_position_agent1 = state[1]['position']\n",
    "    old_position_agent3 = state[3]['position']\n",
    "    if num_no_mov_agent1 > 20:\n",
    "        actions[1] = random.choice(range(0, 5))\n",
    "    if num_no_mov_agent3 > 20:\n",
    "        actions[3] = random.choice(range(0, 5))\n",
    "    state, reward, done, info = env.step(actions)\n",
    "    if abs(state[1]['position'][0]-old_position_agent1[0])+abs(state[1]['position'][1]-old_position_agent1[1]) == 0:\n",
    "        num_no_mov_agent1 += 1\n",
    "    else:\n",
    "        num_no_mov_agent1 = 0\n",
    "    if abs(state[3]['position'][0]-old_position_agent3[0])+abs(state[3]['position'][1]-old_position_agent3[1]) == 0:\n",
    "        num_no_mov_agent3 += 1\n",
    "    else:\n",
    "        num_no_mov_agent3 = 0\n",
    "\n",
    "print(info)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tPower ups per match: 0.10\tAverage length: 11.08\tWin percentage: 0.00\tBombs per match: 0.50\n",
      "Episode 10\tPower ups per match: 0.50\tAverage length: 13.29\tWin percentage: 0.10\tBombs per match: 2.50\n",
      "Episode 20\tPower ups per match: 0.70\tAverage length: 16.90\tWin percentage: 0.00\tBombs per match: 3.40\n",
      "Episode 30\tPower ups per match: 1.30\tAverage length: 21.47\tWin percentage: 0.10\tBombs per match: 5.20\n",
      "Episode 40\tPower ups per match: 2.20\tAverage length: 26.01\tWin percentage: 0.10\tBombs per match: 5.60\n",
      "Episode 50\tPower ups per match: 1.20\tAverage length: 27.99\tWin percentage: 0.30\tBombs per match: 3.80\n",
      "Episode 60\tPower ups per match: 1.60\tAverage length: 31.08\tWin percentage: 0.10\tBombs per match: 5.10\n",
      "Episode 70\tPower ups per match: 1.70\tAverage length: 34.03\tWin percentage: 0.10\tBombs per match: 4.80\n",
      "Episode 80\tPower ups per match: 1.00\tAverage length: 34.42\tWin percentage: 0.20\tBombs per match: 2.70\n",
      "Episode 90\tPower ups per match: 1.20\tAverage length: 35.77\tWin percentage: 0.00\tBombs per match: 3.40\n",
      "Episode 100\tPower ups per match: 2.00\tAverage length: 37.60\tWin percentage: 0.00\tBombs per match: 4.40\n",
      "Episode 110\tPower ups per match: 2.20\tAverage length: 42.02\tWin percentage: 0.10\tBombs per match: 6.00\n",
      "Episode 120\tPower ups per match: 2.00\tAverage length: 46.98\tWin percentage: 0.40\tBombs per match: 7.90\n",
      "Episode 130\tPower ups per match: 3.10\tAverage length: 54.69\tWin percentage: 0.10\tBombs per match: 12.10\n",
      "Episode 140\tPower ups per match: 2.30\tAverage length: 59.44\tWin percentage: 0.00\tBombs per match: 7.20\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    main()\n",
    "    print('Done')\n",
    "except KeyboardInterrupt:\n",
    "    print('Keyboard interrupt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
