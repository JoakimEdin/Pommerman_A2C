{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from . import BaseAgent\n",
    "from .. import characters\n",
    "import pommerman\n",
    "from pommerman import agents\n",
    "import pommerman.characters\n",
    "import pommerman.envs\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "import features\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "def get_variable(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch actor-critic example')\n",
    "parser.add_argument('--gamma', type=float, default=0.99, metavar='G',\n",
    "                    help='discount factor (default: 0.99)')\n",
    "parser.add_argument('--lr', type=float, default=2.5e-4,\n",
    "                    help='learning rate (default: 2.5e-4)')\n",
    "parser.add_argument('--eps', type=float, default=1e-5,\n",
    "                    help='RMSprop optimizer epsilon (default: 1e-5)')\n",
    "parser.add_argument('--alpha', type=float, default=0.99,\n",
    "                    help='RMSprop optimizer apha (default: 0.99)')\n",
    "parser.add_argument('--tau', type=float, default=1.00,\n",
    "                    help='parameter for GAE (default: 1.00)')\n",
    "parser.add_argument('--entropy-coef', type=float, default=0.01,\n",
    "                    help='entropy term coefficient (default: 0.01)')\n",
    "parser.add_argument('--value-loss-coef', type=float, default=0.5,\n",
    "                    help='value loss coefficient (default: 0.5)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='interval between training status logs (default: 10)')\n",
    "parser.add_argument('--num-processes', type=int, default=20,\n",
    "                    help='how many training processes to use (default: 4)')\n",
    "parser.add_argument('--num-steps', type=int, default=20,\n",
    "                    help='number of forward steps in A2C (default: 20)')\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next part is based on [Ross Wightman's work](https://github.com/rwightman/pytorch-pommerman-rl/blob/master/envs/pommerman.py) on the Pommerman challenge where the observations are one hot encoded and compressed. The features function in the buttom takes the observations as input and returns a $9\\times11\\times11$ obervation matrix and $1\\times3$ feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_FEATURE_CONFIG = {\n",
    "    'recode_agents': True,\n",
    "    'compact_powerups': True,\n",
    "    'compact_structure': True,\n",
    "    'rescale': True,\n",
    "}\n",
    "\n",
    "\n",
    "def make_np_float(feature):\n",
    "    return np.array(feature).astype(np.float32)\n",
    "\n",
    "\n",
    "def _rescale(x):\n",
    "    return (x - 0.5) * 2.0\n",
    "\n",
    "\n",
    "def featurize(obs, agent_id, config):\n",
    "    max_item = pommerman.constants.Item.Agent3.value\n",
    "\n",
    "    ob = obs[\"board\"]\n",
    "    ob_bomb_blast_strength = obs[\"bomb_blast_strength\"].astype(np.float32) / pommerman.constants.AGENT_VIEW_SIZE\n",
    "    ob_bomb_life = obs[\"bomb_life\"].astype(np.float32) / pommerman.constants.DEFAULT_BOMB_LIFE\n",
    "\n",
    "    # one hot encode the board items\n",
    "    ob_values = max_item + 1\n",
    "    ob_hot = np.eye(ob_values)[ob]\n",
    "\n",
    "    # replace agent item channels with friend, enemy, self channels\n",
    "    if config['recode_agents']:\n",
    "        self_value = pommerman.constants.Item.Agent0.value + agent_id\n",
    "        enemies = np.logical_and(ob >= pommerman.constants.Item.Agent0.value, ob != self_value)\n",
    "        self = (ob == self_value)\n",
    "        friends = (ob == pommerman.constants.Item.AgentDummy.value)\n",
    "        ob_hot[:, :, 9] = friends.astype(np.float32)\n",
    "        ob_hot[:, :, 10] = self.astype(np.float32)\n",
    "        ob_hot[:, :, 11] = enemies.astype(np.float32)\n",
    "        ob_hot = np.delete(ob_hot, np.s_[12::], axis=2)\n",
    "\n",
    "    if config['compact_powerups']:\n",
    "        # replace powerups with single channel\n",
    "        powerup = ob_hot[:, :, 6] * 0.5 + ob_hot[:, :, 7] * 0.66667 + ob_hot[:, :, 8]\n",
    "        ob_hot[:, :, 6] = powerup\n",
    "        ob_hot = np.delete(ob_hot, [7, 8], axis=2)\n",
    "\n",
    "    # replace bomb item channel with bomb life\n",
    "    ob_hot[:, :, 3] = ob_bomb_life\n",
    "\n",
    "    if config['compact_structure']:\n",
    "        ob_hot[:, :, 0] = 0.5 * ob_hot[:, :, 0] + ob_hot[:, :, 5]  # passage + fog\n",
    "        ob_hot[:, :, 1] = 0.5 * ob_hot[:, :, 2] + ob_hot[:, :, 1]  # rigid + wood walls\n",
    "        ob_hot = np.delete(ob_hot, [2], axis=2)\n",
    "        # replace former fog channel with bomb blast strength\n",
    "        ob_hot[:, :, 5] = ob_bomb_blast_strength\n",
    "    else:\n",
    "        # insert bomb blast strength next to bomb life\n",
    "        ob_hot = np.insert(ob_hot, 4, ob_bomb_blast_strength, axis=2)\n",
    "\n",
    "    self_ammo = make_np_float([obs[\"ammo\"]])\n",
    "    self_blast_strength = make_np_float([obs[\"blast_strength\"]])\n",
    "    self_can_kick = make_np_float([obs[\"can_kick\"]])\n",
    "\n",
    "    ob_hot = ob_hot.transpose((2, 0, 1))  # PyTorch tensor layout compat\n",
    "\n",
    "    if config['rescale']:\n",
    "        ob_hot = _rescale(ob_hot)\n",
    "        self_ammo = _rescale(self_ammo / 10)\n",
    "        self_blast_strength = _rescale(self_blast_strength / pommerman.constants.AGENT_VIEW_SIZE)\n",
    "        self_can_kick = _rescale(self_can_kick)\n",
    "\n",
    "    return [ob_hot], [np.concatenate([self_ammo, self_blast_strength, self_can_kick])]\n",
    "\n",
    "\n",
    "def features(obs, feature_config=DEFAULT_FEATURE_CONFIG):\n",
    "    obs_im, obs_other = featurize(\n",
    "                obs,\n",
    "                0,\n",
    "                feature_config)\n",
    "    return obs_im, obs_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor_Critic(nn.Module):\n",
    "    \"\"\"Actor and critic - networks\"\"\"\n",
    "\n",
    "    def __init__(self, n_inputs, n_outputs, inputs_other, n_conv_output):\n",
    "        super(Policy, self).__init__()\n",
    "        # network\n",
    "        self.CNN = nn.Sequential(\n",
    "                    nn.Conv2d(9, 64, 3, stride=1, padding=1),\n",
    "                    nn.BatchNorm2d(64),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv2d(64, 64, 3, stride=1, padding=1),\n",
    "                    nn.BatchNorm2d(64),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv2d(64, 64, 3, stride=1, padding=1),\n",
    "                    nn.BatchNorm2d(64),\n",
    "                    nn.ReLU(),\n",
    "                    )\n",
    "        self.CNN_mlp = nn.Sequential(        \n",
    "                    nn.Linear(n_conv_output, 1024, bias=True),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(1024, 512, bias=True),\n",
    "                    nn.ReLU(),\n",
    "                    )\n",
    "                    \n",
    "        self.fnn_other = nn.Sequential(\n",
    "                                    nn.Linear(inputs_other, inputs_other, bias=True),\n",
    "                                    nn.ReLU(),\n",
    "                                    )\n",
    "        self.actor = nn.Sequential(\n",
    "                                    nn.Linear(515, 6, bias=False),\n",
    "                                    )\n",
    "        self.state_value = nn.Sequential(\n",
    "                                    nn.Linear(515, 1, bias=False),\n",
    "                                    nn.Tanh(),\n",
    "                                    )\n",
    "        self.GRU = nn.GRUCell(n_inputs, 515)\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.entropies = []\n",
    "        self.log_prob = []\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return get_variable(torch.zeros(1, 515))\n",
    "\n",
    "    def forward(self, x_im, x_other, hxs, batch_size=1):\n",
    "        out = []\n",
    "        x = self.CNN(x_im)\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = self.CNN_mlp(x)\n",
    "        out.append(x)\n",
    "        if batch_size > 1:\n",
    "            x_other = self.input_norm(x_other)\n",
    "        x = self.fnn_other(x_other)\n",
    "        out.append(x)\n",
    "        out = torch.cat(out, dim=1)\n",
    "        out = hxs = self.GRU(out, hxs)\n",
    "        action_scores = self.actor(out)\n",
    "        state_values = self.state_value(out)\n",
    "        return action_scores, state_values[0], hxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PommermanAgent(BaseAgent):\n",
    "    def __init__(self, character=characters.Bomber, mode='old', model=\"saved_models/policy_network_power_up3\"):\n",
    "        super(PytorchAgent_actor_critic, self).__init__(character)\n",
    "\n",
    "        n_inputs = 515\n",
    "        n_conv_output = 7744\n",
    "        inputs_other = 3\n",
    "        n_outputs = 6\n",
    "        \n",
    "        self.net = Actor_Critic(n_inputs, n_outputs, inputs_other, n_conv_output)\n",
    "        \n",
    "        if use_cuda:\n",
    "            self.net = self.net.cuda()  \n",
    "\n",
    "        if mode is not \"new\":\n",
    "            self.net.load_state_dict(torch.load(model))\n",
    "        self.hxs = self.net.init_hidden()\n",
    "\n",
    "    def act(self, obs, action_space=None):\n",
    "        obs_im, obs_other = features(obs)\n",
    "        self.net.eval()\n",
    "        with torch.no_grad():\n",
    "            action_scores, _, hxs = self.net(get_variable(torch.Tensor(obs_im)), get_variable(torch.Tensor(obs_other)), self.hxs)\n",
    "            self.hxs = hxs\n",
    "        return action_scores.argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of agents (exactly four)\n",
    "agent_list = [\n",
    "    PommermanAgent(model=\"saved_models/policy_network_team_agent0\"),\n",
    "    agents.SimpleAgent(),\n",
    "    ommermanAgent(model=\"saved_models/policy_network_team_agent2\"),\n",
    "    agents.SimpleAgent(),\n",
    "]\n",
    "# Make the \"Team\" environment using the agent list\n",
    "env = pommerman.make('PommeTeamCompetitionFast-v0', agent_list)\n",
    "num_episodes = 80000000\n",
    "agent = 2\n",
    "team_agent = 0\n",
    "\n",
    "model = ommermanAgent(mode=\"test\", model=\"saved_models/policy_network_team_agent\"+str(agent)).net\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=args.lr, eps=args.eps, alpha=args.alpha)\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "test_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, hxs):\n",
    "    obs_im, obs_other = features.features(state[agent])\n",
    "    state_im = get_variable(torch.from_numpy(np.array(obs_im)).float())\n",
    "    state_other = get_variable(torch.from_numpy(np.array(obs_other)).float())\n",
    "    action_scores, state_value, hxs = model(state_im, state_other, hxs)\n",
    "    probs = F.softmax(action_scores, dim=-1)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample().detach()\n",
    "    log_prob = m.log_prob(action)\n",
    "    entropy = m.entropy()\n",
    "\n",
    "    model.log_prob.append(log_prob)\n",
    "    model.values.append(state_value)\n",
    "    model.entropies.append(entropy)\n",
    "\n",
    "    return action.item(), hxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_episode(done, state, hxs):\n",
    "    scheduler.step()\n",
    "    entropies = model.entropies\n",
    "    values = model.values\n",
    "    rewards = model.rewards\n",
    "    retain_graph = False\n",
    "    log_probs = model.log_prob\n",
    "    R = get_variable(torch.zeros(1, 1))\n",
    "    if not done:\n",
    "        retain_graph = True\n",
    "        obs_im, obs_other = features.features(state[agent])\n",
    "        state_im = get_variable(torch.from_numpy(np.array(obs_im)).float())\n",
    "        state_other = get_variable(torch.from_numpy(np.array(obs_other)).float())\n",
    "        _, state_value, hxs = model(state_im, state_other, hxs)\n",
    "        R = state_value.detach()\n",
    "\n",
    "    values.append(R)\n",
    "    policy_loss = 0\n",
    "    value_loss = 0\n",
    "    gae = get_variable(torch.zeros(1, 1))\n",
    "\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        R = args.gamma * R + rewards[i]\n",
    "        if R > 1:\n",
    "            R = 1\n",
    "        error = R - values[i]\n",
    "        value_loss = value_loss + 0.5*error.pow(2)\n",
    "\n",
    "        # Generalized Advantage Estimataion\n",
    "        delta_t = rewards[i] + args.gamma * \\\n",
    "            values[i + 1] - values[i]\n",
    "        gae = gae * args.gamma * args.tau + delta_t\n",
    "\n",
    "        policy_loss = policy_loss - \\\n",
    "            log_probs[i] * gae.detach() - args.entropy_coef * entropies[i]\n",
    "    optimizer.zero_grad()\n",
    "    (policy_loss + args.value_loss_coef * value_loss).backward(retain_graph=retain_graph)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "\n",
    "    optimizer.step()\n",
    "    del model.rewards[:]\n",
    "    del model.entropies[:]\n",
    "    del model.values[:]\n",
    "    del model.log_prob[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    win = []\n",
    "    movement = []\n",
    "    number_of_bombs = 0\n",
    "    running_reward = 10\n",
    "    seen_positions = []\n",
    "    power_ups = 0\n",
    "    model.train()\n",
    "    for i_episode in range(num_episodes):\n",
    "        total_steps = 0\n",
    "        state = env.reset()\n",
    "        old_blast_strength = state[agent]['blast_strength']\n",
    "        old_ammo = state[agent]['ammo']\n",
    "        old_can_kick = state[agent]['can_kick']\n",
    "        ammo = 1\n",
    "        hxs = model.init_hidden()\n",
    "        model.zero_grad()\n",
    "        alive0 = 1\n",
    "        alive2 = 1\n",
    "        for t in range(10000):  # Don't infinite loop while learning\n",
    "            for steps in range(args.num_steps):\n",
    "                total_steps += 1\n",
    "                actions = env.act(state)\n",
    "                actions[agent], hxs = select_action(state, hxs)\n",
    "                state, reward, done, _ = env.step(actions)\n",
    "                # region rewardfunction\n",
    "                if alive0 and pommerman.constants.Item.Agent0.value not in state[team_agent]['alive']:\n",
    "                    if agent == 0:\n",
    "                        reward[agent] = -1\n",
    "                        done = 1\n",
    "                    elif agent == 2:\n",
    "                        reward[agent] = -0.5\n",
    "                if alive2 and pommerman.constants.Item.Agent2.value not in state[agent]['alive']:\n",
    "                    if agent == 0:\n",
    "                        reward[agent] = -0.5\n",
    "                    elif agent == 2:\n",
    "                        reward[agent] = -1\n",
    "                        done = 1\n",
    "                if state[agent]['teammate'].value in state[agent]['board']:\n",
    "                    reward[agent] += 0.008\n",
    "                if state[agent]['position'] not in seen_positions:\n",
    "                    seen_positions.append(state[agent]['position'])\n",
    "                    # reward[agent] += 0.002\n",
    "                    movement.append(1)\n",
    "                if actions[agent] == 5 and reward[agent] != -1 and ammo != 0:\n",
    "                    reward[agent] += 0.007\n",
    "                    number_of_bombs += 1\n",
    "                if state[agent]['blast_strength'] > old_blast_strength:\n",
    "                    reward[agent] += 0.8\n",
    "                    power_ups += 1\n",
    "                if state[agent]['ammo'] > old_ammo:\n",
    "                    reward[agent] += 0.8\n",
    "                    power_ups += 1\n",
    "                if state[agent]['can_kick'] > old_can_kick:\n",
    "                    reward[agent] += 0.8\n",
    "                    power_ups += 1\n",
    "                ammo = state[agent]['ammo'] != 0\n",
    "                old_blast_strength = max(state[agent]['blast_strength'], old_blast_strength)\n",
    "                old_ammo = max(state[agent]['ammo'], old_ammo)\n",
    "                old_can_kick = max(state[agent]['can_kick'], old_can_kick)\n",
    "                alive0 = pommerman.constants.Item.Agent0.value in state[team_agent]['alive']\n",
    "                alive2 = pommerman.constants.Item.Agent2.value in state[agent]['alive']\n",
    "                # endregion\n",
    "                model.rewards.append(get_variable(torch.from_numpy(np.array(reward[agent])).float()))\n",
    "                if done or reward[agent] == -1:\n",
    "                    win.append(reward[agent] >= 1)\n",
    "                    seen_positions = []\n",
    "                    done = 1\n",
    "                    break\n",
    "            finish_episode(done, state, hxs)\n",
    "            if done: \n",
    "                break\n",
    "        running_reward = running_reward * 0.99 + total_steps * 0.01\n",
    "        if i_episode % args.log_interval == 0:\n",
    "            print('Episode {}\\tPower ups per match: {:.2f}\\tAverage length: {:.2f}\\tWin percentage: {:.2f}\\tBombs per match: {:.2f}\\tNew Discoveries per match: {:.2f}'.format(\n",
    "                i_episode, power_ups/args.log_interval, running_reward, np.mean(win), number_of_bombs/args.log_interval, sum(movement)/args.log_interval))\n",
    "            win = []\n",
    "            movement = []\n",
    "            power_ups = 0\n",
    "            number_of_bombs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    main()\n",
    "    print('Done')\n",
    "except KeyboardInterrupt:\n",
    "    print('Keyboard interrupt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
